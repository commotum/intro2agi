import json  # Module for handling JSON data
import re  # Module for searching text patterns using regular expressions
from typing import List  # Helps define input/output types for clarity

def load_task(task_path: str):
    """
    Loads a JSON task file.
    Args:
        task_path (str): Path to the JSON file.
    Returns:
        dict: Parsed JSON data as a Python dictionary.
    """
    with open(task_path, 'r') as file:
        return json.load(file)

def count_matching_squares(student_output: List[List[int]], expected_output: List[List[int]]):
    """
    Compares two grids (student's output and expected output) to calculate:
    1. Total number of squares (cells) in the grid.
    2. The number of squares where the student's output matches the expected output.
    Args:
        student_output (List[List[int]]): The grid output generated by the student's code.
        expected_output (List[List[int]]): The correct grid output for comparison.
    Returns:
        tuple: Total squares and matching squares.
    """
    total_squares = 0
    matching_squares = 0

    for i in range(len(expected_output)):
        for j in range(len(expected_output[i])):
            total_squares += 1
            if student_output[i][j] == expected_output[i][j]:
                matching_squares += 1

    return total_squares, matching_squares

def evaluate_code(task_path: str, student_code: str):
    """
    Evaluates the student's code against a specific JSON task.
    Integrates the grading rubric:
    1. Accuracy on provided examples.
    2. Accuracy on hidden test cases.
    3. Generalizability (no hard-coded values).
    Args:
        task_path (str): Path to the JSON task file.
        student_code (str): The student's code as a string.
    Returns:
        dict: Grading results including scores and feedback.
    """
    # Step 1: Load task data
    task = load_task(task_path)

    # Step 2: Simulate running student's code (replace with actual execution logic)
    student_output_visible = task['test'][0]['input']  # Mock; replace with actual code execution
    student_output_hidden = task['test'][0]['input']  # Mock; replace with actual code execution

    # Step 3: Evaluate accuracy on visible examples
    visible_total, visible_match = count_matching_squares(
        student_output_visible, task['test'][0]['output']
    )
    visible_accuracy = visible_match / visible_total

    # Step 4: Evaluate accuracy on hidden test cases (mocked here)
    hidden_total, hidden_match = count_matching_squares(
        student_output_hidden, task['test'][0]['output']
    )
    hidden_accuracy = hidden_match / hidden_total

    # Step 5: Check for hard-coded values using regex
    hard_coded_values = re.findall(r'\b\d+\b', student_code)
    generalizability_score = 2 if not hard_coded_values else (1 if len(hard_coded_values) < 5 else 0)

    # Grading Rubric
    grade = {
        "accuracy_on_examples": 2 if visible_accuracy == 1 else (1 if visible_accuracy >= 0.5 else 0),
        "accuracy_on_hidden": 2 if hidden_accuracy == 1 else (1 if hidden_accuracy >= 0.5 else 0),
        "generalizability": generalizability_score,
    }

    # Feedback
    feedback = {
        "visible_examples_accuracy": f"{visible_match}/{visible_total} squares matched.",
        "hidden_test_accuracy": f"{hidden_match}/{hidden_total} squares matched.",
        "hard_coded_values": f"Found {len(hard_coded_values)} hard-coded values: {hard_coded_values}",
        "rubric_scores": grade,
        "final_score": sum(grade.values()),
        "comments": "Ensure general solutions and test for more complex cases."
    }

    return feedback

if __name__ == "__main__":
    """
    Demonstrates the evaluation process for a sample task and student code.
    """
    # Example task file (update the path as needed)
    task_file = "./data/017c7c7b.json"

    # Example student's code as a string
    student_code = """
    def solve(grid):
        return [[2 if cell == 1 else cell for cell in row] for row in grid]
    """

    # Evaluate the student's code
    results = evaluate_code(task_file, student_code)

    # Print the results
    print("Evaluation Results:")
    for key, value in results.items():
        print(f"{key}: {value}")

