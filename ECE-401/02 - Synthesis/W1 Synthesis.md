# Week 1 Synthesis

## 01 - Micrograd from Scratch

**00:11:10** - This was the first time I heard the derivative described as the **sensitivity** of the response to change. Itâ€™s such a clean, intuitive way to frame the concept. It connects directly to how small changes in input propagate through the system.

I think also neural nets are made out to be way more complicated than they actually are. I mean basically your brain is a neural net, and if you're trying to shoot a basketball there's tons of things you can adjust to see what happens. You can change the moment of release, the amount of follow-through, the amount of force from your legs vs. your arms, the arrangement of your fingers on the ball and the pressure they apply to it, the angle of your legs as you bend down to shoot, the angle of your forearm to your biceps to your shoulder to your body as it moves up and down. Each of these items are things you can tweak, and if you're serious about basketball you think about them consciously and focus on them and their impact. As you take each shot you can adjust your inputs to each of those "parameters" and see what happens. Your brain on its own does the calculus to find out just what needs to be tweaked, by how much, and which actions have more impact than others. Then you simply do it over and over again to collect more information and get closer and closer to making the basket **every time**. That's all backprop is. Find the derivatives of each of the inputs with respect to the final output and voila, there's your adjustments!

## 02 - Makemore from Scratch

_blank for now_



