# LLM Tech Stack Timeline

1. **Gradient Descent**  
   *Method for finding the minimum of a function*  
   - **Cauchy, 1847**  
   [01 - Gradient Descent.pdf](01%20-%20Gradient%20Descent.pdf)

2. **Early Backpropagation Analysis**  
   *Foundations of gradient-based error correction*  
   - **Hadamard, 1908**  
   [02 - Early Backpropagation Analysis.pdf](05%20-%20Hadamard.pdf)  

3. **Word Co-Occurrence Modeling**  
   *Statistical relationships between words*  
   - **Markov, 1913**  
   [03 - Word Co-Occurrence Modeling.pdf](02%20-%20Word%20Co-Occurrence%20Modeling.pdf)

4. **Next-Token Prediction**  
   *Core learning task for language models*  
   - **Shannon, 1948**  
   [04 - Next-Token Prediction.pdf](03%20-%20Next-Token%20Prediction.pdf)

5. **Context Window**  
   *Introduction of limited context for translation*  
   - **Weaver, 1949**  
   [05 - Context Window.pdf](04%20-%20Context%20Window.pdf)

6. **Autodiff and Backpropagation**  
   *Systematic techniques for gradient computation*  
   - **Linnainmaa, 1970**  
   [06 - Autodiff and Backpropagation.pdf](05%20-%20Autodiff%20%26%20Backpropagation.pdf)

7. **Word Embeddings: Probabilistic Models**  
   *Early neural network-based language models*  
   - **Bengio, 2003**  
   [07 - Word Embeddings.pdf](06%20-%20Word%20Embeddings.pdf)

8. **Word Embeddings: Vector Space Models**  
   *Continuous word representations with efficiency*  
   - **Mikolov, 2013**  
   [08 - Words in Vector Space.pdf](06%20-%20Words%20in%20Vector%20Space.pdf)

9. **Adam Optimization Algorithm**  
   *Adaptive moment estimation for optimization*  
   - **Kingma et al., 2014**  
   [09 - Optimization Algorithm.pdf](07%20-%20Optimization%20Algorithm.pdf)

10. **Transformer Architecture**  
    *Attention-based neural network design*  
    - **Vaswani et al., 2017**  
    [10 - Transformers.pdf](08%20-%20Transformers.pdf)
